{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting morfessor\n",
      "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
      "Installing collected packages: morfessor\n",
      "Successfully installed morfessor-2.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "from polyglot.text import Text\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = os.path.join(\"../data\", \"item1\")\n",
    "## Encode UTF-u and remove non-printable characters\n",
    "#    document = filter(\n",
    "#        lambda char: char in string.printable,\n",
    "#        unicodedata.normalize('NFKD', document.decode('utf-8'))\n",
    "#    )\n",
    "kddcorpus = nltk.corpus.PlaintextCorpusReader(CORPUS, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyglot_entities(fileids=None, section = None, corpus=kddcorpus):\n",
    "    \"\"\"\n",
    "    Extract entities from each file using polyglot\n",
    "    \"\"\"\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    fileids = fileids or corpus.fileids()\n",
    "\n",
    "    for fileid in fileids:\n",
    "        if section is not None:\n",
    "            text = Text((list(sectpull([fileid],section=section))[0][1]))\n",
    "        else:\n",
    "            text = Text(corpus.raw(fileid))\n",
    "\n",
    "\n",
    "\n",
    "        for entity in text.entities:\n",
    "            etext = \" \".join(entity)\n",
    "\n",
    "            if entity.tag == 'I-PER':\n",
    "                key = 'persons'\n",
    "            elif entity.tag == 'I-ORG':\n",
    "                key = 'organizations'\n",
    "            elif entity.tag == 'I-locations':\n",
    "                key = 'locations'\n",
    "            else:\n",
    "                key = 'other'\n",
    "\n",
    "            results[fileid][key].append(etext)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanford_entities(model, jar, fileids=None, corpus=kddcorpus, section = None):\n",
    "    \"\"\"\n",
    "    Extract entities using the Stanford NER tagger.\n",
    "    Must pass in the path to the tagging model and jar as downloaded from the\n",
    "    Stanford Core NLP website.\n",
    "    \"\"\"\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    fileids = fileids or corpus.fileids()\n",
    "    tagger  = StanfordNERTagger(model, jar)\n",
    "    section = section\n",
    "\n",
    "    for fileid in fileids:\n",
    "        if section is not None:\n",
    "            text = nltk.word_tokenize(list(sectpull([fileid],section=section))[0][1])\n",
    "        else:\n",
    "            text  = corpus.words(fileid)\n",
    "\n",
    "        chunk = []\n",
    "\n",
    "        for token, tag in tagger.tag(text):\n",
    "            if tag == 'O':\n",
    "                if chunk:\n",
    "                    # Flush the current chunk\n",
    "                    etext =  \" \".join([c[0] for c in chunk])\n",
    "                    etag  = chunk[0][1]\n",
    "                    chunk = []\n",
    "\n",
    "                    if etag == 'PERSON':\n",
    "                        key = 'persons'\n",
    "                    elif etag == 'ORGANIZATION':\n",
    "                        key = 'organizations'\n",
    "                    elif etag == 'LOCATION':\n",
    "                        key = 'locations'\n",
    "                    else:\n",
    "                        key = 'other'\n",
    "\n",
    "                    results[fileid][key].append(etext)\n",
    "\n",
    "            else:\n",
    "                # Build chunk from tags\n",
    "                chunk.append((token, tag))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_entities(fileids=None, section = None,corpus=kddcorpus):\n",
    "    \"\"\"\n",
    "    Extract entities using the NLTK named entity chunker.\n",
    "    \"\"\"\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    fileids = fileids or corpus.fileids()\n",
    "\n",
    "    for fileid in fileids:\n",
    "        if section is not None:\n",
    "            text = nltk.pos_tag(nltk.word_tokenize(list(sectpull([fileid],section=section))[0][1]))\n",
    "        else:\n",
    "            text = nltk.pos_tag(corpus.words(fileid))\n",
    "\n",
    "\n",
    "\n",
    "        for entity in nltk.ne_chunk(text):\n",
    "            if isinstance(entity, nltk.tree.Tree):\n",
    "                etext = \" \".join([word for word, tag in entity.leaves()])\n",
    "                label = entity.label()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if label == 'PERSON':\n",
    "                key = 'persons'\n",
    "            elif label == 'ORGANIZATION':\n",
    "                key = 'organizations'\n",
    "            elif label == 'LOCATION':\n",
    "                key = 'locations'\n",
    "            elif label == 'GPE':\n",
    "                key = 'other'\n",
    "            else:\n",
    "                key = None\n",
    "\n",
    "            if key:\n",
    "                results[fileid][key].append(etext)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sectpull' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8b0e41a8b61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# NLTK Entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnltkents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'top'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Polyglot Entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-2c31e088d148>\u001b[0m in \u001b[0;36mnltk_entities\u001b[0;34m(fileids, section, corpus)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msectpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sectpull' is not defined"
     ]
    }
   ],
   "source": [
    "# Only extract our annotated files.\n",
    "fids  = ['msft-item1-2018.txt', 'goog-item1-2016.txt']\n",
    "\n",
    "# NLTK Entities\n",
    "nltkents = nltk_entities(fids, section='top')\n",
    "\n",
    "# Polyglot Entities\n",
    "polyents = polyglot_entities(fids, section='top')\n",
    "\n",
    "# Stanford Model Loading\n",
    "root  = os.path.expanduser('~/models/stanford-ner-2014-01-04/')\n",
    "model = os.path.join(root, 'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "jar   = os.path.join(root, 'stanford-ner-2014-01-04.jar')\n",
    "\n",
    "# Stanford Entities\n",
    "stanents = stanford_entities(model, jar, fids, section='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##See https://www.districtdatalabs.com/named-entity-recognition-and-classification-for-entity-extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
